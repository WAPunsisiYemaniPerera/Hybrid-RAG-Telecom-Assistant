# ğŸ“¡ Telecom Support Assistant  
### Smart AI Helpdesk for Telecom Operators

An **AI-powered Telecom Support Assistant** built using **Hybrid Retrieval-Augmented Generation (RAG)**.  
It answers customer support questions by first searching **internal telecom PDF manuals** and intelligently **falls back to live web search** when the required information is not available in the documents.

Built with **Streamlit**, **LangChain**, **Google Gemini**, **FAISS**, and **Tavily Web Search**.

---

## ğŸŒŸ Highlights

- ğŸ” Semantic search across multiple telecom PDFs  
- ğŸ¤– AI-generated answers using Google Gemini  
- ğŸŒ Automatic web search fallback (Tavily)  
- ğŸ’¬ Chat-style Streamlit interface  
- âš¡ Fast and efficient retrieval using FAISS  

---

## ğŸ§  Architecture

**Hybrid RAG + Web Search Fallback**

User Question  
â†“  
PDF Vector Store (FAISS)  
â†“  
Relevant Context Found?  
â”œâ”€ Yes â†’ Gemini â†’ Answer  
â””â”€ No â†’ Tavily Web Search â†’ Gemini â†’ Answer

This design:
- Prioritizes trusted internal knowledge
- Reduces hallucinations
- Ensures helpful answers even when PDFs lack data

---

## ğŸ“ Project Structure

.  
â”œâ”€â”€ app.py â€” Main Streamlit application  
â”œâ”€â”€ requirements.txt â€” Base dependencies  
â”œâ”€â”€ data/ â€” Telecom PDF manuals  
â”‚   â”œâ”€â”€ Lanka_Telecom_Data_2025.pdf  
â”‚   â””â”€â”€ Tariff_2022_Dialog.pdf  
â””â”€â”€ .env â€” API keys (ignored by Git)

---

## ğŸš€ Features

- ğŸ“„ **Multi-PDF ingestion** and semantic retrieval  
- ğŸ§© **Chunking + embeddings** for accurate search  
- ğŸ¤– **Google Gemini LLM** for natural language answers  
- ğŸŒ **Tavily web search fallback**  
- ğŸ’¬ **Persistent chat history**  
- ğŸ–¥ï¸ Clean and simple **Streamlit UI**

---

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**  
- **Streamlit** â€“ Frontend UI  
- **LangChain** â€“ RAG orchestration  
- **FAISS** â€“ Vector database  
- **HuggingFace Embeddings**  
- **Google Gemini** â€“ Large Language Model  
- **Tavily** â€“ Web search API  
- **python-dotenv** â€“ Environment variable handling

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\\Scripts\\activate
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
pip install streamlit python-dotenv google-generativeai \
	langchain_community langchain_text_splitters \
	langchain_huggingface faiss-cpu huggingface-hub
```

### âš™ï¸ Configuration

Create a `.env` file in the project root:

```env
GOOGLE_API_KEY=your_google_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
```

âš ï¸ The application will stop if these keys are missing.

### â–¶ï¸ Running the Application

Place telecom PDF manuals inside the `data/` folder

Start the Streamlit app:

```bash
streamlit run app.py
```

Open the browser URL shown (usually http://localhost:8501)

Ask your telecom-related questions ğŸ§

---

## ğŸ” How It Works

- PDFs are loaded using `PyPDFLoader`  
- Documents are split into chunks  
- Embeddings are generated using HuggingFace models  
- FAISS stores vectors for fast similarity search  
- Gemini generates answers from retrieved context  
- If no relevant context exists â†’ Tavily web search is used

---

## ğŸ§¯ Troubleshooting

- Missing API keys â†’ Check `.env` file  
- No PDFs found â†’ Ensure files are placed in `data/`  
- FAISS installation issues â†’ Use `faiss-cpu` unless GPU support is configured

---

## ğŸ§© Future Enhancements

- Dependency version pinning  
- Confidence scoring for answers  
- Cloud deployment (Streamlit Cloud / HuggingFace Spaces)  
- Authentication for enterprise use  
- CI/CD pipeline integration  
- UI improvements and analytics dashboard

---

## ğŸ“Œ Project Summary (CV / LinkedIn Ready)

Developed a Hybrid RAG-based Telecom Support Assistant that answers customer queries using internal PDF manuals with an intelligent web search fallback, built using Streamlit, LangChain, Google Gemini, and FAISS.

â­ If you find this project useful, please star the repository and feel free to contribute!

## Summary

This project loads PDF manuals from the `data/` folder, builds an embeddings-based vector store, and uses Google Gemini (via `google.generativeai`) plus LangChain utilities to answer user questions in a chat-like Streamlit UI.

## Features

- Search and answer questions using uploaded PDF guides (vector search + LLM generation)
- Fallback web search using Tavily when answers are not found in the internal docs
- Simple, clean Streamlit UI with chat history

## Files

- [app.py](app.py) â€” main Streamlit app
- [requirements.txt](requirements.txt) â€” environment dependencies (may be incomplete; see below)
- [data/](data/) â€” put PDF manuals here (example files present: [data/Lanka_Telecom_Data_2025.pdf](data/Lanka_Telecom_Data_2025.pdf), [data/Tariff_2022_Dialog.pdf](data/Tariff_2022_Dialog.pdf))

## Requirements

- Python 3.8+ recommended
- See `requirements.txt` for base packages. The app additionally requires the following packages (inferred from imports):

```
streamlit
python-dotenv
google-generativeai
langchain_community
langchain_text_splitters
langchain_huggingface
faiss-cpu
huggingface-hub
```

Install dependencies (example):

```bash
python -m venv .venv
source .venv/bin/activate   # on Windows use: .venv\\Scripts\\activate
pip install -r requirements.txt
pip install streamlit python-dotenv google-generativeai langchain_community langchain_text_splitters langchain_huggingface faiss-cpu huggingface-hub
```

## Configuration

Create a `.env` file at the project root with the following keys:

```
GOOGLE_API_KEY=your_google_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
```

The app will stop and show an error if the keys are missing.

## Usage

Start the Streamlit app locally:

```bash
streamlit run app.py
```

Open the local URL printed by Streamlit (usually http://localhost:8501).

Place PDF files you want the assistant to use into the `data/` folder before starting the app. The app currently looks for `.pdf` files in that folder and will build a FAISS vectorstore from the documents.

## How it works (high level)

- `app.py` loads PDFs via `PyPDFLoader`, splits documents into chunks, creates embeddings with `HuggingFaceEmbeddings`, and builds a FAISS index.
- A Google Gemini model is used to generate answers from retrieved context chunks. If the model returns `NOT_FOUND`, a Tavily web search is attempted as a fallback.

## Troubleshooting

- If you see `Keys not found` make sure `.env` exists and the two API keys are set.
- If no PDFs are found, create a `data/` folder and add PDFs.
- FAISS and some embedding/model libraries may require platform-specific installs (e.g., `faiss-cpu` vs `faiss-gpu`).

## Next steps / Suggestions

- Add a `requirements-extras.txt` that pins all LLM and LangChain-related packages used by the app.
- Add a CI workflow to lint and run basic checks.
- Add CONTRIBUTING.md and LICENSE if this will be shared publicly.

---

If you want, I can: run the app locally, create a `.env.example`, or pin the exact dependencies into `requirements.txt`.
